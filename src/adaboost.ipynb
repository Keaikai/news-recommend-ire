{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "import re,os\n",
    "import json\n",
    "import ast,nltk\n",
    "import datetime\n",
    "from stemming.porter2 import stem\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import logging\n",
    "import time\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "# Articles that has category given\n",
    "with open('articles.json','r') as data_file:\n",
    "    line = data_file.readline()\n",
    "    while line:\n",
    "        line = ast.literal_eval(line)\n",
    "        training_data.append(line)\n",
    "        line = data_file.readline()\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data = []\n",
    "# Articles that has no category given\n",
    "with open('articles_wo.json','r') as data_file:\n",
    "    line = data_file.readline()\n",
    "    while line:\n",
    "        line = ast.literal_eval(line)\n",
    "        testing_data.append(line)\n",
    "        line = data_file.readline()\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse sentences to words and remove stopwords from sentence\n",
    "def sentence_to_wordlist( sentence, remove_stopwords=True ):\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",sentence)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse text to sentences using tokenizer mentioned above\n",
    "def text_to_sentences(text,tokenizer,remove_stopwords=True):\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( sentence_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories_dict = {'automobiles' : 0, 'business' : 1, 'computing' : 2, 'criminals' : 3, 'entertainment' : 4, 'fashion' : 5,\\\n",
    "                  'foods' : 6, 'health' : 7, 'lifestyle' : 8, 'pbusiness' : 9, 'pfashion' : 10, 'phealth' : 11,\\\n",
    "                  'plifestyle' : 12, 'politics' : 13, 'ptechnology' : 14, 'ptravel' : 15, 'science' : 16, 'sports' : 17,\\\n",
    "                  'technology' : 18, 'travel' : 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words = []\n",
    "classes = []\n",
    "documents = []\n",
    "#no_classes = []\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# loop through each sentence in our training data\n",
    "for pattern in training_data:\n",
    "    # tokenize each word in the sentence\n",
    "    #words.extend(nltk.word_tokenize(pattern['desc']))\n",
    "    documents += [[stem(str(word)) for word in sentence] \\\n",
    "                        for sentence in [text_to_sentences(pattern['desc'], tokenizer)]]\n",
    "    classes.append(categories_dict[pattern['category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pattern in testing_data:\n",
    "    documents += [[stem(str(word)) for word in sentence] \\\n",
    "                          for sentence in [text_to_sentences(pattern['desc'],tokenizer)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(documents):\n",
    "    # Import the built-in logging module and configure it so that Word2Vec \n",
    "    # creates nice output messages\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "        level=logging.INFO)\n",
    "\n",
    "    # Set values for various parameters\n",
    "    num_features = 300    # Word vector dimensionality                      \n",
    "    min_word_count = 2  # Minimum word count                        \n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 4           # Context window size                                                                                    \n",
    "    downsampling = 1e-5   # Downsample setting for frequent words\n",
    "    # Initialize and train the model (this will take some time)\n",
    "\n",
    "    print \"Training model...\"\n",
    "    model = Word2Vec(documents, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=False)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model_name = \"300features_2minwords_4context\"\n",
    "    model.save(model_name)\n",
    "    #model.save_word2vec_format(model_name,binary=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words,model,num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float128\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #sorted_set = set(model.sort_vocab())\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float128\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "    # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    "\n",
    "    # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "    # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-31 15:24:53,794 : INFO : collecting all words and their counts\n",
      "2017-10-31 15:24:53,796 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-10-31 15:24:53,957 : INFO : PROGRESS: at sentence #10000, processed 204500 words, keeping 182852 word types\n",
      "2017-10-31 15:24:53,969 : INFO : collected 190351 word types from a corpus of 213395 raw words and 10852 sentences\n",
      "2017-10-31 15:24:53,970 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-31 15:24:54,182 : INFO : min_count=2 retains 14692 unique words (7% of original 190351, drops 175659)\n",
      "2017-10-31 15:24:54,184 : INFO : min_count=2 leaves 37736 word corpus (17% of original 213395, drops 175659)\n",
      "2017-10-31 15:24:54,248 : INFO : deleting the raw counts dictionary of 190351 items\n",
      "2017-10-31 15:24:54,255 : INFO : sample=0.001 downsamples 2 most-common words\n",
      "2017-10-31 15:24:54,257 : INFO : downsampling leaves estimated 36502 word corpus (96.7% of prior 37736)\n",
      "2017-10-31 15:24:54,258 : INFO : estimated required memory for 14692 words and 300 dimensions: 42606800 bytes\n",
      "2017-10-31 15:24:54,324 : INFO : resetting layer weights\n",
      "2017-10-31 15:24:54,569 : INFO : training model with 4 workers on 14692 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2017-10-31 15:24:54,570 : INFO : expecting 10852 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-10-31 15:24:55,163 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-10-31 15:24:55,165 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-10-31 15:24:55,167 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-10-31 15:24:55,172 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-10-31 15:24:55,173 : INFO : training on 1066975 raw words (182546 effective words) took 0.6s, 309842 effective words/s\n",
      "2017-10-31 15:24:55,175 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-10-31 15:24:55,205 : INFO : saving Word2Vec object under 300features_2minwords_4context, separately None\n",
      "2017-10-31 15:24:55,207 : INFO : not storing attribute syn0norm\n",
      "2017-10-31 15:24:55,208 : INFO : not storing attribute cum_table\n",
      "2017-10-31 15:24:55,479 : INFO : saved 300features_2minwords_4context\n",
      "/home/dell/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10852\n",
      "Review 1000 of 10852\n",
      "Review 2000 of 10852\n",
      "Review 3000 of 10852\n",
      "Review 4000 of 10852\n",
      "Review 5000 of 10852\n",
      "Review 6000 of 10852\n",
      "Review 7000 of 10852\n",
      "Review 8000 of 10852\n",
      "Review 9000 of 10852\n",
      "Review 10000 of 10852\n"
     ]
    }
   ],
   "source": [
    "model = build_model(documents)\n",
    "DataVecs = getAvgFeatureVecs( documents, model, 300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = Imputer().fit_transform(DataVecs[:10369])\n",
    "Y_train = np.array(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_model = build_model(test_documents)\n",
    "#TestDataVecs = getAvgFeatureVecs(test_documents,test_model,300)\n",
    "X_test = Imputer().fit_transform(DataVecs[10369:])\n",
    "#print X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bdt_real = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),n_estimators=600,learning_rate=1.5,algorithm=\"SAMME\")\n",
    "bdt_real.fit(X_train, Y_train)\n",
    "Y_test = bdt_real.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "computing\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "computing\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "travel\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "computing\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n",
      "fashion\n"
     ]
    }
   ],
   "source": [
    "for category in predictions:\n",
    "    print categories_dict.keys()[int(category)]\n",
    "#print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.24384760e-03  -8.47065938e-04   1.89919946e-03   1.42334960e-04\n",
      "  -2.17245036e-03  -1.59490202e-03   9.85398423e-04   1.65537160e-03\n",
      "   7.76215136e-04  -1.92861352e-03   1.44635812e-03   5.24335541e-04\n",
      "  -4.12896392e-03   3.75122228e-03  -2.30566271e-03   3.55496269e-03\n",
      "   1.27599348e-03  -3.65242362e-03  -5.84987225e-04  -1.73114933e-03\n",
      "   1.80758057e-03  -2.76248436e-03  -9.16886667e-04   3.50919156e-03\n",
      "   8.95717472e-04  -1.19994834e-03  -2.60477688e-03  -7.71814230e-04\n",
      "  -3.16000881e-03  -8.57082894e-04  -4.99296002e-05   3.73968773e-03\n",
      "  -2.54886091e-03   2.06646742e-04   2.77502247e-03   2.07541842e-03\n",
      "   1.26061076e-03   9.52588860e-04   7.49359257e-04   2.44473334e-03\n",
      "   2.38667859e-03  -4.46007616e-04   2.72727321e-03  -4.23072453e-03\n",
      "   2.39738310e-03   2.46637396e-03   3.21137975e-03   2.58726068e-04\n",
      "  -1.65540478e-03   1.01002050e-03   3.95442569e-03  -2.16846392e-03\n",
      "  -3.52181832e-03  -4.52782493e-04   3.73213075e-03  -2.38172198e-03\n",
      "   1.36116287e-05   8.03649076e-04   4.83457756e-04   3.83988477e-03\n",
      "   1.67506290e-03   1.59620133e-04   1.58090526e-03   1.53381520e-03\n",
      "   1.91480821e-03   6.56087534e-04  -2.50116200e-03  -7.28294486e-04\n",
      "   3.17936158e-03   2.52501290e-03   7.25492951e-04   2.84436106e-03\n",
      "  -5.72542311e-04  -2.46486795e-03  -1.32368156e-03  -8.83590081e-04\n",
      "  -2.25993613e-03   3.41717340e-03  -2.53016048e-03  -2.05185660e-03\n",
      "   1.30994026e-03   2.72798404e-03  -1.31402438e-03  -3.60061333e-03\n",
      "  -2.69771018e-03   2.12752685e-03   2.22983578e-03  -5.61933266e-05\n",
      "   5.94572397e-04  -1.54292933e-03   2.59413209e-04   1.42938286e-03\n",
      "  -7.66511512e-04  -4.76933387e-03   1.26696019e-03  -3.19370476e-04\n",
      "  -1.03335828e-04  -1.59442931e-03   2.26693553e-03  -4.25712205e-04]\n"
     ]
    }
   ],
   "source": [
    "print X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_error_rate(pred, Y):\n",
    "    return sum(pred != Y) / float(len(Y))\n",
    "\n",
    "def generic_clf(Y_train, X_train, X_test, clf):\n",
    "    clf.fit(X_train,Y_train)\n",
    "    pred_train = clf.predict(X_train)\n",
    "    pred_test = clf.predict(X_test)\n",
    "    #return get_error_rate(pred_train, Y_train), \\\n",
    "    #    get_error_rate(pred_test, Y_test)\n",
    "    return pred_test\n",
    "    \n",
    "def adaboost_clf(Y_train, X_train, X_test, M, clf):\n",
    "    n_train, n_test = len(X_train), len(X_test)\n",
    "    # Initialize weights\n",
    "    w = np.ones(n_train) / n_train\n",
    "    pred_train, pred_test = [np.zeros(n_train), np.zeros(n_test)]\n",
    "    \n",
    "    for i in range(M):\n",
    "        # Fit a classifier with the specific weights\n",
    "        clf.fit(X_train, Y_train, sample_weight = w)\n",
    "        pred_train_i = clf.predict(X_train)\n",
    "        pred_test_i = clf.predict(X_test)\n",
    "        # Indicator function\n",
    "        miss = [int(x) for x in (pred_train_i != Y_train)]\n",
    "        # Equivalent with 1/-1 to update weights\n",
    "        miss2 = [x if x==1 else -1 for x in miss]\n",
    "        # Error\n",
    "        err_m = np.dot(w,miss) / sum(w)\n",
    "        # Alpha\n",
    "        alpha_m = 0.5 * np.log( (1 - err_m) / float(err_m))\n",
    "        # New weights\n",
    "        w = np.multiply(w, np.exp([float(x) * alpha_m for x in miss2]))\n",
    "        # Add to prediction\n",
    "        pred_train = [sum(x) for x in zip(pred_train, \n",
    "                                          [x * alpha_m for x in pred_train_i])]\n",
    "        pred_test = [sum(x) for x in zip(pred_test, \n",
    "                                         [x * alpha_m for x in pred_test_i])]\n",
    "    \n",
    "    pred_train, pred_test = np.sign(pred_train), np.sign(pred_test)\n",
    "    # Return error rate in train and test set\n",
    "    #return get_error_rate(pred_train, Y_train), \\\n",
    "    #       get_error_rate(pred_test, Y_test)\n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_tree = DecisionTreeClassifier(max_depth = 1, random_state = 1)\n",
    "pred_test = adaboost_clf(Y_train, X_train, X_test, 50, clf_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print pred_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
