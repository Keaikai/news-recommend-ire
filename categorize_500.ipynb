{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re,os\n",
    "import json\n",
    "import ast,nltk\n",
    "import datetime\n",
    "from stemming.porter2 import stem\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "# Articles that has category given\n",
    "with open('articles.json','r') as data_file:\n",
    "    line = data_file.readline()\n",
    "    while line:\n",
    "        line = ast.literal_eval(line)\n",
    "        training_data.append(line)\n",
    "        line = data_file.readline()\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data = []\n",
    "# Articles that has no category given\n",
    "with open('articles_wo.json','r') as data_file:\n",
    "    line = data_file.readline()\n",
    "    while line:\n",
    "        line = ast.literal_eval(line)\n",
    "        testing_data.append(line)\n",
    "        line = data_file.readline()\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10369\n"
     ]
    }
   ],
   "source": [
    "print len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse sentences to words and remove stopwords from sentence\n",
    "def sentence_to_wordlist( sentence, remove_stopwords=True ):\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",sentence)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse text to sentences using tokenizer mentioned above\n",
    "def text_to_sentences(text,tokenizer,remove_stopwords=True):\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( sentence_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "no_classes = []\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# loop through each sentence in our training data\n",
    "for pattern in training_data:\n",
    "    # tokenize each word in the sentence\n",
    "    words.extend(nltk.word_tokenize(pattern['desc']))\n",
    "    documents += [[stem(str(word)) for word in sentence] \\\n",
    "                        for sentence in [text_to_sentences(pattern['desc'], tokenizer)]]\n",
    "    classes.append(pattern['category'])\n",
    "    if pattern['category'] not in no_classes:\n",
    "        no_classes.append(pattern['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_documents = []\n",
    "for pattern in testing_data:\n",
    "    documents += [[stem(str(word)) for word in sentence] \\\n",
    "                          for sentence in [text_to_sentences(pattern['desc'],tokenizer)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(no_classes)\n",
    "for Class in classes:\n",
    "    output_row = list(output_empty)\n",
    "    output_row[no_classes.index(Class)] = 1\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "words = [stem(w.lower()) for w in words if w not in stops]\n",
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80053\n"
     ]
    }
   ],
   "source": [
    "print len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-31 15:50:27,238 : INFO : collecting all words and their counts\n",
      "2017-10-31 15:50:27,239 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-10-31 15:50:27,414 : INFO : PROGRESS: at sentence #10000, processed 204500 words, keeping 182852 word types\n",
      "2017-10-31 15:50:27,427 : INFO : collected 190351 word types from a corpus of 213395 raw words and 10852 sentences\n",
      "2017-10-31 15:50:27,428 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-31 15:50:28,432 : INFO : min_count=1 retains 190351 unique words (100% of original 190351, drops 0)\n",
      "2017-10-31 15:50:28,433 : INFO : min_count=1 leaves 213395 word corpus (100% of original 213395, drops 0)\n",
      "2017-10-31 15:50:28,990 : INFO : deleting the raw counts dictionary of 190351 items\n",
      "2017-10-31 15:50:28,998 : INFO : sample=0.001 downsamples 1 most-common words\n",
      "2017-10-31 15:50:28,999 : INFO : downsampling leaves estimated 212687 word corpus (99.7% of prior 213395)\n",
      "2017-10-31 15:50:29,001 : INFO : estimated required memory for 190351 words and 100 dimensions: 247456300 bytes\n",
      "2017-10-31 15:50:29,646 : INFO : resetting layer weights\n",
      "2017-10-31 15:50:31,864 : INFO : training model with 4 workers on 190351 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2017-10-31 15:50:31,865 : INFO : expecting 10852 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-10-31 15:50:32,883 : INFO : PROGRESS: at 63.39% examples, 670627 words/s, in_qsize 6, out_qsize 0\n",
      "2017-10-31 15:50:33,387 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-10-31 15:50:33,399 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-10-31 15:50:33,415 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-10-31 15:50:33,423 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-10-31 15:50:33,425 : INFO : training on 1066975 raw words (1063448 effective words) took 1.6s, 685676 effective words/s\n",
      "2017-10-31 15:50:33,426 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-10-31 15:50:33,533 : INFO : saving Word2Vec object under 100features_1minwords_4context, separately None\n",
      "2017-10-31 15:50:33,534 : INFO : not storing attribute syn0norm\n",
      "2017-10-31 15:50:33,535 : INFO : storing np array 'syn0' to 100features_1minwords_4context.wv.syn0.npy\n",
      "2017-10-31 15:50:35,754 : INFO : storing np array 'syn1neg' to 100features_1minwords_4context.syn1neg.npy\n",
      "2017-10-31 15:50:36,286 : INFO : not storing attribute cum_table\n",
      "2017-10-31 15:50:39,872 : INFO : saved 100features_1minwords_4context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 1  # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 4           # Context window size                                                                                    \n",
    "downsampling = 1e-5   # Downsample setting for frequent words\n",
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "print \"Training model...\"\n",
    "model = Word2Vec(documents, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=False)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"100features_1minwords_4context\"\n",
    "model.save(model_name)\n",
    "#model.save_word2vec_format(model_name,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words,model,num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float128\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #sorted_set = set(model.sort_vocab())\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float128\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "    # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    "\n",
    "    # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "    # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10852\n",
      "Review 1000 of 10852\n",
      "Review 2000 of 10852\n",
      "Review 3000 of 10852\n",
      "Review 4000 of 10852\n",
      "Review 5000 of 10852\n",
      "Review 6000 of 10852\n",
      "Review 7000 of 10852\n",
      "Review 8000 of 10852\n",
      "Review 9000 of 10852\n",
      "Review 10000 of 10852\n"
     ]
    }
   ],
   "source": [
    "DataVecs = getAvgFeatureVecs( documents, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10852, 100)\n"
     ]
    }
   ],
   "source": [
    "print DataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = Imputer().fit_transform(DataVecs[:len(training_data)])\n",
    "X_test = Imputer().fit_transform(DataVecs[len(training_data):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    " \n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "def think(x, show_details=False):\n",
    "    #x = bow(sentence.lower(), words, show_details)\n",
    "    #if show_details:\n",
    "    #    print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our bag of words\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = sigmoid(np.dot(l0, synapse_0))\n",
    "    # output layer\n",
    "    l2 = sigmoid(np.dot(l1, synapse_1))\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANN and Gradient Descent code from https://iamtrask.github.io//2015/07/27/python-network-part2/\n",
    "def train(X, y, hidden_neurons=10, alpha=1, epochs=50000, dropout=False, dropout_percent=0.5):\n",
    "\n",
    "    print (\"Training with %s neurons, alpha:%s, dropout:%s %s\" % (hidden_neurons, str(alpha), dropout, dropout_percent if dropout else '') )\n",
    "    print (\"Input matrix: %sx%s    Output matrix: %sx%s\" % (len(X),len(X[0]),1, len(no_classes)) )\n",
    "    np.random.seed(1)\n",
    "\n",
    "    last_mean_error = 1\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((len(X[0]), hidden_neurons)) - 1\n",
    "    synapse_1 = 2*np.random.random((hidden_neurons, len(no_classes))) - 1\n",
    "\n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "\n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "        \n",
    "    for j in iter(range(epochs+1)):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
    "                \n",
    "        if(dropout):\n",
    "            layer_1 *= np.random.binomial([np.ones((len(X),hidden_neurons))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = y - layer_2\n",
    "\n",
    "        if (j% 10000) == 0 and j > 5000:\n",
    "            # if this 10k iteration's error is greater than the last iteration, break out\n",
    "            if np.mean(np.abs(layer_2_error)) < last_mean_error:\n",
    "                print (\"delta after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))) )\n",
    "                last_mean_error = np.mean(np.abs(layer_2_error))\n",
    "            else:\n",
    "                print (\"break:\", np.mean(np.abs(layer_2_error)), \">\", last_mean_error )\n",
    "                break\n",
    "                \n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "        if(j > 0):\n",
    "            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))        \n",
    "        \n",
    "        synapse_1 += alpha * synapse_1_weight_update\n",
    "        synapse_0 += alpha * synapse_0_weight_update\n",
    "        \n",
    "        prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "        prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # persist synapses\n",
    "    synapse = {'synapse0': synapse_0.tolist(), 'synapse1': synapse_1.tolist(),\n",
    "               'datetime': now.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "               'words': words,\n",
    "               'classes': no_classes\n",
    "              }\n",
    "    synapse_file = \"synapses.json\"\n",
    "\n",
    "    with open(synapse_file, 'w') as outfile:\n",
    "        json.dump(synapse, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", synapse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 10 neurons, alpha:0.1, dropout:False \n",
      "Input matrix: 10369x100    Output matrix: 1x20\n",
      "delta after 10000 iterations:0.0692545086315\n",
      "('saved synapses to:', 'synapses.json')\n",
      "('processing time:', 326.55551195144653, 'seconds')\n"
     ]
    }
   ],
   "source": [
    "#X = np.array(DataVecs)\n",
    "y = np.array(output)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train(X_train, y, hidden_neurons=10, alpha=0.1, epochs=10000, dropout=False, dropout_percent=0.1)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"processing time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.01\n",
    "# load our calculated synapse values\n",
    "synapse_file = 'synapses.json' \n",
    "with open(synapse_file) as data_file: \n",
    "    synapse = json.load(data_file) \n",
    "    synapse_0 = np.asarray(synapse['synapse0']) \n",
    "    synapse_1 = np.asarray(synapse['synapse1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(vec, show_details=False):\n",
    "    results = think(vec, show_details)\n",
    "    #print results\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[no_classes[r[0]],r[1]] for r in results]\n",
    "    #print (\"%s \\n classification: %s\" % (sentence, return_results))\n",
    "    return return_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics\n"
     ]
    }
   ],
   "source": [
    "print classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e+000   7.36052383e-128   1.52489876e-046   8.28115100e-151\n",
      "   3.83233949e-144   7.19932234e-120   3.50048849e-127   4.14029635e-140\n",
      "   4.49421595e-163   2.10519511e-137   2.54433141e-062   3.73151491e-155\n",
      "   1.13974292e-137   7.86277584e-052   1.38256933e-165   1.51050887e-097\n",
      "   6.47299382e-056   1.78673886e-155   4.25125898e-164   5.96559822e-126]\n",
      "[['politics', 1.0]]\n"
     ]
    }
   ],
   "source": [
    "for vec in X_test:\n",
    "    res = classify(X_test[0])\n",
    "    print res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
