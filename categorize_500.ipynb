{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re,os\n",
    "import json\n",
    "import ast,nltk\n",
    "import datetime\n",
    "from stemming.porter2 import stem\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "from gensim.models import Word2Vec,KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "# Articles that has category given\n",
    "with open('articles.json','r') as data_file:\n",
    "    line = data_file.readline()\n",
    "    while line:\n",
    "        line = ast.literal_eval(line)\n",
    "        training_data.append(line)\n",
    "        line = data_file.readline()\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data = []\n",
    "# Articles that has no category given\n",
    "with open('articles_wo.json','r') as data_file:\n",
    "    line = data_file.readline()\n",
    "    while line:\n",
    "        line = ast.literal_eval(line)\n",
    "        testing_data.append(line)\n",
    "        line = data_file.readline()\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10369\n"
     ]
    }
   ],
   "source": [
    "print len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse sentences to words and remove stopwords from sentence\n",
    "def sentence_to_wordlist( sentence, remove_stopwords=True ):\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",sentence)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse text to sentences using tokenizer mentioned above\n",
    "def text_to_sentences(text,tokenizer,remove_stopwords=True):\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( sentence_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "no_classes = []\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# loop through each sentence in our training data\n",
    "for pattern in training_data:\n",
    "    # tokenize each word in the sentence\n",
    "    words.extend(nltk.word_tokenize(pattern['desc']))\n",
    "    documents += [[stem(str(word)) for word in sentence] \\\n",
    "                        for sentence in [text_to_sentences(pattern['desc'], tokenizer)]]\n",
    "    classes.append(pattern['category'])\n",
    "    if pattern['category'] not in no_classes:\n",
    "        no_classes.append(pattern['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_documents = []\n",
    "for pattern in testing_data:\n",
    "    test_documents += [[stem(str(word)) for word in sentence] \\\n",
    "                          for sentence in [text_to_sentences(pattern['desc'],tokenizer)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(no_classes)\n",
    "for Class in classes:\n",
    "    output_row = list(output_empty)\n",
    "    output_row[no_classes.index(Class)] = 1\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "words = [stem(w.lower()) for w in words if w not in stops]\n",
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80053\n"
     ]
    }
   ],
   "source": [
    "print len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-29 18:57:59,931 : INFO : collecting all words and their counts\n",
      "2017-10-29 18:57:59,933 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-10-29 18:58:00,106 : INFO : PROGRESS: at sentence #10000, processed 204500 words, keeping 182852 word types\n",
      "2017-10-29 18:58:00,116 : INFO : collected 188628 word types from a corpus of 211322 raw words and 10369 sentences\n",
      "2017-10-29 18:58:00,117 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-29 18:58:01,191 : INFO : min_count=1 retains 188628 unique words (100% of original 188628, drops 0)\n",
      "2017-10-29 18:58:01,192 : INFO : min_count=1 leaves 211322 word corpus (100% of original 211322, drops 0)\n",
      "2017-10-29 18:58:01,686 : INFO : deleting the raw counts dictionary of 188628 items\n",
      "2017-10-29 18:58:01,692 : INFO : sample=0.001 downsamples 1 most-common words\n",
      "2017-10-29 18:58:01,696 : INFO : downsampling leaves estimated 210615 word corpus (99.7% of prior 211322)\n",
      "2017-10-29 18:58:01,702 : INFO : estimated required memory for 188628 words and 100 dimensions: 245216400 bytes\n",
      "2017-10-29 18:58:02,222 : INFO : resetting layer weights\n",
      "2017-10-29 18:58:04,685 : INFO : training model with 4 workers on 188628 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2017-10-29 18:58:04,686 : INFO : expecting 10369 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-10-29 18:58:05,693 : INFO : PROGRESS: at 54.89% examples, 576417 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-29 18:58:06,408 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-10-29 18:58:06,413 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-10-29 18:58:06,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-10-29 18:58:06,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-10-29 18:58:06,438 : INFO : training on 1056610 raw words (1053119 effective words) took 1.7s, 603190 effective words/s\n",
      "2017-10-29 18:58:06,439 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-10-29 18:58:06,545 : INFO : saving Word2Vec object under 100features_1minwords_4context, separately None\n",
      "2017-10-29 18:58:06,546 : INFO : not storing attribute syn0norm\n",
      "2017-10-29 18:58:06,548 : INFO : storing np array 'syn0' to 100features_1minwords_4context.wv.syn0.npy\n",
      "2017-10-29 18:58:06,739 : INFO : storing np array 'syn1neg' to 100features_1minwords_4context.syn1neg.npy\n",
      "2017-10-29 18:58:06,813 : INFO : not storing attribute cum_table\n",
      "2017-10-29 18:58:09,284 : INFO : saved 100features_1minwords_4context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 1  # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 4           # Context window size                                                                                    \n",
    "downsampling = 1e-5   # Downsample setting for frequent words\n",
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "print \"Training model...\"\n",
    "model = Word2Vec(documents, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=False)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"100features_1minwords_4context\"\n",
    "model.save(model_name)\n",
    "#model.save_word2vec_format(model_name,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words,model,num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float128\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #sorted_set = set(model.sort_vocab())\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float128\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "    # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    "\n",
    "    # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "    # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10369\n",
      "Review 1000 of 10369\n",
      "Review 2000 of 10369\n",
      "Review 3000 of 10369\n",
      "Review 4000 of 10369\n",
      "Review 5000 of 10369\n",
      "Review 6000 of 10369\n",
      "Review 7000 of 10369\n",
      "Review 8000 of 10369\n",
      "Review 9000 of 10369\n",
      "Review 10000 of 10369\n"
     ]
    }
   ],
   "source": [
    "DataVecs = getAvgFeatureVecs( documents, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00017184154  2.4503889e-05  0.00012210625 -0.00015397034 -0.0015202732\n",
      "  0.00028750111  0.0012024901  0.0011574512 -0.0013270631 -0.00064087883\n",
      "  0.00028746979  0.0001006107 -0.00079584038  0.0015622324  8.0731538e-05\n",
      "  0.0010361368  0.00036334324  0.00018084297 -0.0014883342 -0.0019142148\n",
      "  0.0010670282  1.2763992e-06  5.8317633e-05  7.0355972e-05 -0.0010568126\n",
      " -0.00021687347 -0.00106138 -0.00053426733 -0.0014644218 -0.0017388701\n",
      "  0.00020616575  0.0011127155 -0.0005813003  0.00012722127  0.00088584367\n",
      "  0.00097818095 -0.00044892763  0.00040844048 -0.0010613813  0.00087935478\n",
      "  0.0009803304 -0.00076347655  0.0014898952 -0.00033581083  0.0007813529\n",
      " -9.2976384e-05  0.00032211664  0.00072148183  0.00043570729 -0.00040095301\n",
      "  0.00023414203  0.00069072985  7.26954e-05 -0.00051793239 -2.1585842e-05\n",
      " -0.00081479376 -0.00016346795  6.9226186e-05  0.00047237841 -0.00029495278\n",
      "  0.00051753635  0.00036791475  0.0020165471  0.00083393433  0.0011918705\n",
      " -0.00038207743 -9.9803669e-05  0.00037605349  0.001730072  0.0012263998\n",
      " -0.001066673  2.7369491e-05  0.00077918304  0.0002762435  8.4190867e-05\n",
      "  0.001114157 -0.0014224013  0.0013638671 -0.00063433038  0.00023929268\n",
      "  0.0018475504  0.00030608424 -0.0016327405 -0.00097257489 -0.00012540367\n",
      "  0.00052117523  0.00083383206 -0.00079748699 -0.0010698288  0.0003890369\n",
      "  0.00083861046 -1.68174e-05  0.00034747293 -0.0012189358 -0.00027431968\n",
      " -0.00088350443 -0.001676168 -0.00050010579 -0.00012084216  3.4882477e-05]\n"
     ]
    }
   ],
   "source": [
    "print DataVecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    " \n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "def think(x, show_details=False):\n",
    "    #x = bow(sentence.lower(), words, show_details)\n",
    "    #if show_details:\n",
    "    #    print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our bag of words\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = sigmoid(np.dot(l0, synapse_0))\n",
    "    # output layer\n",
    "    l2 = sigmoid(np.dot(l1, synapse_1))\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANN and Gradient Descent code from https://iamtrask.github.io//2015/07/27/python-network-part2/\n",
    "def train(X, y, hidden_neurons=10, alpha=1, epochs=50000, dropout=False, dropout_percent=0.5):\n",
    "\n",
    "    print (\"Training with %s neurons, alpha:%s, dropout:%s %s\" % (hidden_neurons, str(alpha), dropout, dropout_percent if dropout else '') )\n",
    "    print (\"Input matrix: %sx%s    Output matrix: %sx%s\" % (len(X),len(X[0]),1, len(no_classes)) )\n",
    "    np.random.seed(1)\n",
    "\n",
    "    last_mean_error = 1\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((len(X[0]), hidden_neurons)) - 1\n",
    "    synapse_1 = 2*np.random.random((hidden_neurons, len(no_classes))) - 1\n",
    "\n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "\n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "        \n",
    "    for j in iter(range(epochs+1)):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
    "                \n",
    "        if(dropout):\n",
    "            layer_1 *= np.random.binomial([np.ones((len(X),hidden_neurons))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = y - layer_2\n",
    "\n",
    "        if (j% 10000) == 0 and j > 5000:\n",
    "            # if this 10k iteration's error is greater than the last iteration, break out\n",
    "            if np.mean(np.abs(layer_2_error)) < last_mean_error:\n",
    "                print (\"delta after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))) )\n",
    "                last_mean_error = np.mean(np.abs(layer_2_error))\n",
    "            else:\n",
    "                print (\"break:\", np.mean(np.abs(layer_2_error)), \">\", last_mean_error )\n",
    "                break\n",
    "                \n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "        if(j > 0):\n",
    "            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))        \n",
    "        \n",
    "        synapse_1 += alpha * synapse_1_weight_update\n",
    "        synapse_0 += alpha * synapse_0_weight_update\n",
    "        \n",
    "        prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "        prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # persist synapses\n",
    "    synapse = {'synapse0': synapse_0.tolist(), 'synapse1': synapse_1.tolist(),\n",
    "               'datetime': now.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "               'words': words,\n",
    "               'classes': no_classes\n",
    "              }\n",
    "    synapse_file = \"synapses.json\"\n",
    "\n",
    "    with open(synapse_file, 'w') as outfile:\n",
    "        json.dump(synapse, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", synapse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 10 neurons, alpha:0.1, dropout:False \n",
      "Input matrix: 10369x100    Output matrix: 1x20\n",
      "('saved synapses to:', 'synapses.json')\n",
      "('processing time:', 146.79171299934387, 'seconds')\n"
     ]
    }
   ],
   "source": [
    "X = np.array(DataVecs)\n",
    "y = np.array(output)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train(X, y, hidden_neurons=10, alpha=0.1, epochs=1000, dropout=False, dropout_percent=0.1)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"processing time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-29 19:16:06,620 : INFO : collecting all words and their counts\n",
      "2017-10-29 19:16:06,621 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-10-29 19:16:06,625 : INFO : collected 1903 word types from a corpus of 2073 raw words and 483 sentences\n",
      "2017-10-29 19:16:06,627 : INFO : Loading a fresh vocabulary\n",
      "2017-10-29 19:16:06,634 : INFO : min_count=1 retains 1903 unique words (100% of original 1903, drops 0)\n",
      "2017-10-29 19:16:06,636 : INFO : min_count=1 leaves 2073 word corpus (100% of original 2073, drops 0)\n",
      "2017-10-29 19:16:06,644 : INFO : deleting the raw counts dictionary of 1903 items\n",
      "2017-10-29 19:16:06,650 : INFO : sample=0.001 downsamples 6 most-common words\n",
      "2017-10-29 19:16:06,652 : INFO : downsampling leaves estimated 2018 word corpus (97.4% of prior 2073)\n",
      "2017-10-29 19:16:06,654 : INFO : estimated required memory for 1903 words and 100 dimensions: 2473900 bytes\n",
      "2017-10-29 19:16:06,660 : INFO : resetting layer weights\n",
      "2017-10-29 19:16:06,687 : INFO : training model with 4 workers on 1903 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2017-10-29 19:16:06,688 : INFO : expecting 483 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-10-29 19:16:06,705 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-10-29 19:16:06,708 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-10-29 19:16:06,710 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-10-29 19:16:06,723 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-10-29 19:16:06,724 : INFO : training on 10365 raw words (10091 effective words) took 0.0s, 350431 effective words/s\n",
      "2017-10-29 19:16:06,725 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-10-29 19:16:06,728 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-10-29 19:16:06,730 : INFO : saving Word2Vec object under 100testfeatures_1minwords_4context, separately None\n",
      "2017-10-29 19:16:06,731 : INFO : not storing attribute syn0norm\n",
      "2017-10-29 19:16:06,733 : INFO : not storing attribute cum_table\n",
      "2017-10-29 19:16:06,771 : INFO : saved 100testfeatures_1minwords_4context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 1  # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 4           # Context window size                                                                                    \n",
    "downsampling = 1e-5   # Downsample setting for frequent words\n",
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "print \"Training model...\"\n",
    "testmodel = Word2Vec(test_documents, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "testmodel.init_sims(replace=False)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"100testfeatures_1minwords_4context\"\n",
    "testmodel.save(model_name)\n",
    "#model.save_word2vec_format(model_name,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 483\n"
     ]
    }
   ],
   "source": [
    "TestDataVecs = getAvgFeatureVecs( test_documents, testmodel, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0006890658 -0.00076296009 -0.0029413692  0.0013572672  0.0037287914\n",
      " -0.0013011911 -5.1862997e-05 -0.00037936732  0.0011542493  0.00092733761\n",
      "  0.00039084792  0.00098856304 -0.00011817757  0.0015670626  0.00030479493\n",
      "  0.00073972395 -0.0013632241 -0.00051237282  0.00082528099 -0.0004652932\n",
      " -0.00096308258  0.0010061627 -0.00016638165 -0.0013457348 -0.0023903108\n",
      "  0.0015798633 -0.00073414524  0.002382225  0.0020401925 -0.00055952312\n",
      "  0.001046675 -0.00058265467 -0.00016036895  0.0012069821 -0.0027204474\n",
      " -0.00081853732 -0.0011815737  0.0026908009  0.00046523407 -0.0017699723\n",
      " -0.0024348643 -0.00051858962 -0.0017020455  0.0016795258  0.00082494505\n",
      " -0.00040003934 -0.00031471313  0.0012815975 -0.0011005206 -0.0010705766\n",
      "  0.0010046149 -0.0012450011  0.00061226469 -0.0017370856 -7.8674566e-06\n",
      "  0.00047870026  0.0014340116  0.001262779  0.00072022204 -0.00025013371\n",
      " -0.0015941704 -0.00015020691  0.00030866565 -0.00082557416 -0.00013252324\n",
      "  0.00018200482 -0.00066850494  0.00026254755  0.0015868368 -0.00034374357\n",
      "  0.001039932 -0.0029130167 -0.0025845361 -0.0017747109 -0.00094536226\n",
      " -0.0024281112  0.00078817064  0.00042265242  3.7175982e-05  0.0035251181\n",
      "  0.0027880675 -0.0011080949  0.00044849922 -0.002036317 -0.0011487207\n",
      "  0.0020116076 -0.0016417083 -0.0014580272  0.00011548765 -0.00039882469\n",
      "  1.9965617e-05 -0.00073669288 -0.00062491812  0.0012554523  0.0016229736\n",
      "  0.00066738026 -0.0028087616  0.0014699241  0.00010425849  0.00055964949]\n"
     ]
    }
   ],
   "source": [
    "print TestDataVecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.01\n",
    "# load our calculated synapse values\n",
    "synapse_file = 'synapses.json' \n",
    "with open(synapse_file) as data_file: \n",
    "    synapse = json.load(data_file) \n",
    "    synapse_0 = np.asarray(synapse['synapse0']) \n",
    "    synapse_1 = np.asarray(synapse['synapse1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(vec, show_details=False):\n",
    "    results = think(vec, show_details)\n",
    "    print results\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[no_classes[r[0]],r[1]] for r in results]\n",
    "    #print (\"%s \\n classification: %s\" % (sentence, return_results))\n",
    "    return return_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics\n"
     ]
    }
   ],
   "source": [
    "print classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0  7.4535529e-128  1.4285547e-46  9.3041586e-151  4.4277408e-144\n",
      "  8.6185771e-120  3.8687555e-127  4.854048e-140  5.2323548e-163\n",
      "  2.2749179e-137  2.2924973e-62  4.0771911e-155  1.2615845e-137\n",
      "  7.3228435e-52  1.619236e-165  1.5891255e-97  6.3797803e-56\n",
      "  2.1281178e-155  4.9241489e-164  6.4979743e-126]\n",
      "[['politics', 0.99999999999999999946]]\n"
     ]
    }
   ],
   "source": [
    "res = classify(TestDataVecs[0])\n",
    "print res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
