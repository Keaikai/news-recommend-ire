{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re,os\n",
    "import json\n",
    "import ast,nltk\n",
    "import datetime\n",
    "from stemming.porter2 import stem\n",
    "from collections import defaultdict, Counter\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "import operator\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from sklearn.preprocessing import Imputer\n",
    "#from blagging import BlaggingClassifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import model_selection, metrics,cross_validation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "# Articles that has category given\n",
    "with open('articles.json','r') as data_file:\n",
    "    line = data_file.readline()\n",
    "    while line:\n",
    "        line = ast.literal_eval(line)\n",
    "        training_data.append(line)\n",
    "        line = data_file.readline()\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data = []\n",
    "# Articles that has no category given\n",
    "with open('articles_wo.json','r') as data_file:\n",
    "    line = data_file.readline()\n",
    "    while line:\n",
    "        line = ast.literal_eval(line)\n",
    "        testing_data.append(line)\n",
    "        line = data_file.readline()\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse sentences to words and remove stopwords from sentence\n",
    "def sentence_to_wordlist( sentence, remove_stopwords=True ):\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",sentence)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "# function to parse text to sentences using tokenizer mentioned above\n",
    "def text_to_sentences(text,tokenizer,remove_stopwords=True):\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( sentence_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories_dict = {'automobiles' : 1, 'business' : 2, 'computing' : 3, 'criminals' : 4, 'entertainment' : 5, 'fashion' : 6,\\\n",
    "                  'foods' : 7, 'health' : 8, 'lifestyle' : 9, 'pbusiness' : 10, 'pfashion' : 11, 'phealth' : 12,\n",
    "                  'plifestyle' : 13, 'politics' : 14, 'ptechnology' : 15, 'ptravel' : 16, 'science' : 17, 'sports' : 18,\n",
    "                  'technology' : 19, 'travel' : 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#words = []\n",
    "classes = []\n",
    "documents = []\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# loop through each sentence in our training data\n",
    "for pattern in training_data:\n",
    "    # tokenize each word in the sentence\n",
    "    #words.extend(nltk.word_tokenize(pattern['desc']))\n",
    "    documents += [[stem(str(word)) for word in sentence] \\\n",
    "                        for sentence in [text_to_sentences(pattern['desc'], tokenizer)]]\n",
    "    classes.append(categories_dict[pattern['category']])\n",
    "    #if pattern['category'] not in no_classes:\n",
    "    #    no_classes.append(pattern['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pattern in testing_data:\n",
    "    documents += [[stem(str(word)) for word in sentence] \\\n",
    "                          for sentence in [text_to_sentences(pattern['desc'],tokenizer)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    14 :    3188  =  30.75%\n",
      "     5 :    1684  =  16.24%\n",
      "    18 :    1487  =  14.34%\n",
      "     2 :     972  =  9.37%\n",
      "     4 :     776  =  7.48%\n",
      "    19 :     516  =  4.98%\n",
      "     8 :     397  =  3.83%\n",
      "    20 :     318  =  3.07%\n",
      "     3 :     302  =  2.91%\n",
      "     1 :     163  =  1.57%\n",
      "     6 :     158  =  1.52%\n",
      "    17 :     157  =  1.51%\n",
      "     9 :     130  =  1.25%\n",
      "     7 :      90  =  0.87%\n",
      "    10 :      15  =  0.14%\n",
      "    13 :      10  =  0.10%\n",
      "    11 :       2  =  0.02%\n",
      "    15 :       2  =  0.02%\n",
      "    12 :       1  =  0.01%\n",
      "    16 :       1  =  0.01%\n"
     ]
    }
   ],
   "source": [
    "def class_info(classes):\n",
    "    counts = Counter(classes)\n",
    "    total = sum(counts.values())\n",
    "    #print counts\n",
    "    counts = sorted(counts.items(), key=operator.itemgetter(1),reverse = True)\n",
    "    for cls in counts:\n",
    "        print \"%6s : % 7d  =  %0.2f%%\" %(cls[0],cls[1],float(cls[1])/total*100)\n",
    "        \n",
    "class_info(classes)\n",
    "#print classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10852\n"
     ]
    }
   ],
   "source": [
    "print len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_classes = []\n",
    "new_documents = []\n",
    "for i in range(len(training_data)):\n",
    "    if classes[i] not in [11,15,12,16]:\n",
    "        new_documents.append(documents[i])\n",
    "        if classes[i] == 13:\n",
    "            freq_classes.append(11)\n",
    "        elif classes[i] == 14:\n",
    "            freq_classes.append(12)\n",
    "        elif classes[i] == 17:\n",
    "            freq_classes.append(13)\n",
    "        elif classes[i] == 18:\n",
    "            freq_classes.append(14)\n",
    "        elif classes[i] == 19:\n",
    "            freq_classes.append(15)\n",
    "        elif classes[i] == 20:\n",
    "            freq_classes.append(16)\n",
    "        else:\n",
    "            freq_classes.append(classes[i])\n",
    "for i in range(len(training_data),len(documents)):\n",
    "    new_documents.append(documents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10846\n",
      "10363\n"
     ]
    }
   ],
   "source": [
    "print len(new_documents)\n",
    "print len(freq_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-04 11:16:08,653 : INFO : collecting all words and their counts\n",
      "2017-11-04 11:16:08,655 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-04 11:16:08,818 : INFO : PROGRESS: at sentence #10000, processed 204514 words, keeping 182898 word types\n",
      "2017-11-04 11:16:08,830 : INFO : collected 190316 word types from a corpus of 213321 raw words and 10846 sentences\n",
      "2017-11-04 11:16:08,831 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-04 11:16:09,112 : INFO : min_count=2 retains 14673 unique words (7% of original 190316, drops 175643)\n",
      "2017-11-04 11:16:09,114 : INFO : min_count=2 leaves 37678 word corpus (17% of original 213321, drops 175643)\n",
      "2017-11-04 11:16:09,168 : INFO : deleting the raw counts dictionary of 190316 items\n",
      "2017-11-04 11:16:09,174 : INFO : sample=0.001 downsamples 2 most-common words\n",
      "2017-11-04 11:16:09,175 : INFO : downsampling leaves estimated 36444 word corpus (96.7% of prior 37678)\n",
      "2017-11-04 11:16:09,176 : INFO : estimated required memory for 14673 words and 100 dimensions: 19074900 bytes\n",
      "2017-11-04 11:16:09,240 : INFO : resetting layer weights\n",
      "2017-11-04 11:16:09,403 : INFO : training model with 4 workers on 14673 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2017-11-04 11:16:09,404 : INFO : expecting 10846 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-11-04 11:16:09,842 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-04 11:16:09,843 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-04 11:16:09,844 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-04 11:16:09,846 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-04 11:16:09,848 : INFO : training on 1066605 raw words (182175 effective words) took 0.4s, 419843 effective words/s\n",
      "2017-11-04 11:16:09,856 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-11-04 11:16:09,865 : INFO : saving Word2Vec object under 100features_2minwords_4context, separately None\n",
      "2017-11-04 11:16:09,866 : INFO : not storing attribute syn0norm\n",
      "2017-11-04 11:16:09,868 : INFO : not storing attribute cum_table\n",
      "2017-11-04 11:16:10,041 : INFO : saved 100features_2minwords_4context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 100\n",
    "# Word vector dimensionality                      \n",
    "min_word_count = 2  # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 4           # Context window size                                                                                    \n",
    "downsampling = 1e-5   # Downsample setting for frequent words\n",
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "print \"Training model...\"\n",
    "model = Word2Vec(documents, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=False)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"100features_2minwords_4context\"\n",
    "model.save(model_name)\n",
    "#model.save_word2vec_format(model_name,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words,model,num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float128\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #sorted_set = set(model.sort_vocab())\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float128\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "    # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    "\n",
    "    # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "    # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 10846\n",
      "Review 2000 of 10846\n",
      "Review 3000 of 10846\n",
      "Review 4000 of 10846\n",
      "Review 5000 of 10846\n",
      "Review 6000 of 10846\n",
      "Review 7000 of 10846\n",
      "Review 8000 of 10846\n",
      "Review 9000 of 10846\n",
      "Review 10000 of 10846\n"
     ]
    }
   ],
   "source": [
    "DataVecs = getAvgFeatureVecs( new_documents, model, num_features )\n",
    "DataVecs = Imputer().fit_transform(DataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(DataVecs[:10363])\n",
    "X_test = np.array(DataVecs[10363:])\n",
    "Y_train = np.array(freq_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 3.9735429447852759, 2: 0.66634516460905346, 3: 2.14466059602649, 4: 0.83464884020618557, 5: 0.3846125296912114, 6: 4.099287974683544, 7: 7.1965277777777779, 8: 1.6314546599496222, 9: 4.9822115384615389, 10: 43.179166666666667, 11: 64.768749999999997, 12: 0.20316420953575909, 13: 4.1253980891719744, 14: 0.43556657700067247, 15: 1.2552083333333333, 16: 2.0367531446540879}\n"
     ]
    }
   ],
   "source": [
    "class_wt = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\n",
    "class_wt = dict(enumerate(class_wt.flatten(), 1))\n",
    "print class_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.088873878220592492"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(min_samples_leaf=5,class_weight=class_wt)\n",
    "predicted = model_selection.cross_val_predict(clf, X_train, Y_train, cv=5)\n",
    "metrics.accuracy_score(Y_train, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
